2023-12-13 23:30:54 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2023-12-13 23:30:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 99130, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': True, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/jprado/experiments/fairseq_dadaptation_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 55, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.01], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoint_d0_1e-0_99130', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=99130, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=True, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='/home/jprado/experiments/fairseq_dadaptation_module', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='dadapt-adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm_wiseman_iwslt_de_en', max_epoch=55, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.01], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoint_d0_1e-0_99130', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=True, share_all_embeddings=False, data='data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_dropout_in=0, encoder_dropout_out=0, decoder_embed_dim=256, decoder_out_embed_dim=256, decoder_dropout_in=0, decoder_dropout_out=0.3, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=256, encoder_layers=1, encoder_bidirectional=False, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=256, decoder_layers=1, decoder_attention='1', adaptive_softmax_cutoff='10000,50000,200000', _name='lstm_wiseman_iwslt_de_en'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': Namespace(no_progress_bar=False, log_interval=100, log_format='json', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=99130, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=True, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir='/home/jprado/experiments/fairseq_dadaptation_module', empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='dadapt-adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm_wiseman_iwslt_de_en', max_epoch=55, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.01], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoint_d0_1e-0_99130', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=True, share_all_embeddings=False, data='data-bin/iwslt14.tokenized.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.3, no_seed_provided=False, encoder_embed_dim=256, encoder_dropout_in=0, encoder_dropout_out=0, decoder_embed_dim=256, decoder_out_embed_dim=256, decoder_dropout_in=0, decoder_dropout_out=0.3, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=256, encoder_layers=1, encoder_bidirectional=False, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=256, decoder_layers=1, decoder_attention='1', adaptive_softmax_cutoff='10000,50000,200000', _name='dadapt-adam'), 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.01]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-12-13 23:30:55 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2023-12-13 23:30:55 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2023-12-13 23:30:56 | INFO | fairseq_cli.train | LSTMModel(
  (encoder): LSTMEncoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 256, padding_idx=1)
    (lstm): LSTM(256, 256)
  )
  (decoder): LSTMDecoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 256, padding_idx=1)
    (layers): ModuleList(
      (0): LSTMCell(512, 256)
    )
    (attention): AttentionLayer(
      (input_proj): Linear(in_features=256, out_features=256, bias=False)
      (output_proj): Linear(in_features=512, out_features=256, bias=False)
    )
  )
)
2023-12-13 23:30:56 | INFO | fairseq_cli.train | task: TranslationTask
2023-12-13 23:30:56 | INFO | fairseq_cli.train | model: LSTMModel
2023-12-13 23:30:56 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-12-13 23:30:56 | INFO | fairseq_cli.train | num. shared model params: 5,474,304 (num. trained: 5,474,304)
2023-12-13 23:30:56 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-12-13 23:30:56 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2023-12-13 23:30:56 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2023-12-13 23:30:56 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2023-12-13 23:30:57 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias
2023-12-13 23:30:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-12-13 23:30:57 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.392 GB ; name = NVIDIA A100-SXM4-40GB                   
2023-12-13 23:30:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2023-12-13 23:30:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2023-12-13 23:30:57 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2023-12-13 23:30:57 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoint_d0_1e-0_99130/checkpoint_last.pt
2023-12-13 23:30:57 | INFO | fairseq.trainer | No existing checkpoint found checkpoint_d0_1e-0_99130/checkpoint_last.pt
2023-12-13 23:30:57 | INFO | fairseq.trainer | loading train data for epoch 1
2023-12-13 23:30:57 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2023-12-13 23:30:57 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2023-12-13 23:30:57 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
Using decoupled weight decay
2023-12-13 23:30:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:30:57 | INFO | fairseq.trainer | begin training epoch 1
2023-12-13 23:30:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:31:04 | INFO | train_inner | {"epoch": 1, "update": 0.091, "loss": "11.539", "nll_loss": "11.349", "ppl": "2607.74", "wps": "55551.4", "ups": "15.34", "wpb": "3625.1", "bsz": "142.3", "num_updates": "100", "lr": "0.00025", "gnorm": "0.581", "loss_scale": "128", "train_wall": "7", "gb_free": "38.9", "wall": "7"}
2023-12-13 23:31:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-12-13 23:31:10 | INFO | train_inner | {"epoch": 1, "update": 0.182, "loss": "9.937", "nll_loss": "9.452", "ppl": "700.15", "wps": "60144.7", "ups": "16.66", "wpb": "3611", "bsz": "143.3", "num_updates": "200", "lr": "0.0005", "gnorm": "0.729", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "13"}
2023-12-13 23:31:16 | INFO | train_inner | {"epoch": 1, "update": 0.273, "loss": "9.621", "nll_loss": "9.097", "ppl": "547.49", "wps": "63878.5", "ups": "17.67", "wpb": "3615.2", "bsz": "156.1", "num_updates": "300", "lr": "0.00075", "gnorm": "0.794", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "19"}
2023-12-13 23:31:22 | INFO | train_inner | {"epoch": 1, "update": 0.364, "loss": "9.338", "nll_loss": "8.775", "ppl": "438.09", "wps": "55364.7", "ups": "15.77", "wpb": "3510.9", "bsz": "147.4", "num_updates": "400", "lr": "0.001", "gnorm": "0.683", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "25"}
2023-12-13 23:31:28 | INFO | train_inner | {"epoch": 1, "update": 0.455, "loss": "8.894", "nll_loss": "8.262", "ppl": "306.98", "wps": "58406.9", "ups": "16.33", "wpb": "3577", "bsz": "149.7", "num_updates": "500", "lr": "0.00125", "gnorm": "0.642", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "31"}
2023-12-13 23:31:34 | INFO | train_inner | {"epoch": 1, "update": 0.545, "loss": "8.597", "nll_loss": "7.91", "ppl": "240.54", "wps": "59013.5", "ups": "16.62", "wpb": "3550.2", "bsz": "142.5", "num_updates": "600", "lr": "0.0015", "gnorm": "0.542", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "37"}
2023-12-13 23:31:40 | INFO | train_inner | {"epoch": 1, "update": 0.636, "loss": "8.295", "nll_loss": "7.558", "ppl": "188.5", "wps": "68029.7", "ups": "19", "wpb": "3579.7", "bsz": "148.2", "num_updates": "700", "lr": "0.00175", "gnorm": "0.497", "loss_scale": "64", "train_wall": "5", "gb_free": "38.9", "wall": "42"}
2023-12-13 23:31:46 | INFO | train_inner | {"epoch": 1, "update": 0.727, "loss": "8.123", "nll_loss": "7.355", "ppl": "163.76", "wps": "57272.2", "ups": "16", "wpb": "3579.8", "bsz": "126.2", "num_updates": "800", "lr": "0.002", "gnorm": "0.513", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "49"}
2023-12-13 23:31:51 | INFO | train_inner | {"epoch": 1, "update": 0.818, "loss": "7.651", "nll_loss": "6.811", "ppl": "112.3", "wps": "65759", "ups": "18.24", "wpb": "3604.6", "bsz": "142.2", "num_updates": "900", "lr": "0.00225", "gnorm": "0.476", "loss_scale": "64", "train_wall": "5", "gb_free": "38.9", "wall": "54"}
2023-12-13 23:31:57 | INFO | train_inner | {"epoch": 1, "update": 0.908, "loss": "7.25", "nll_loss": "6.344", "ppl": "81.23", "wps": "68805.9", "ups": "19.01", "wpb": "3619.7", "bsz": "155.7", "num_updates": "1000", "lr": "0.0025", "gnorm": "0.479", "loss_scale": "64", "train_wall": "5", "gb_free": "38.9", "wall": "59"}
2023-12-13 23:32:02 | INFO | train_inner | {"epoch": 1, "update": 0.999, "loss": "7.026", "nll_loss": "6.079", "ppl": "67.6", "wps": "61230.6", "ups": "17.25", "wpb": "3549.2", "bsz": "140.1", "num_updates": "1100", "lr": "0.00275", "gnorm": "0.489", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "65"}
2023-12-13 23:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:32:04 | INFO | valid | {"epoch": 1, "valid_loss": "6.703", "valid_nll_loss": "5.721", "valid_ppl": "52.73", "valid_wps": "120308", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "1101"}
2023-12-13 23:32:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1101 updates
2023-12-13 23:32:04 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint1.pt
2023-12-13 23:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint1.pt
2023-12-13 23:32:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint1.pt (epoch 1 @ 1101 updates, score 6.703) (writing took 0.17136579472571611 seconds)
2023-12-13 23:32:04 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-12-13 23:32:04 | INFO | train | {"epoch": 1, "train_loss": "8.754", "train_nll_loss": "8.092", "train_ppl": "272.86", "train_wps": "59278.5", "train_ups": "16.54", "train_wpb": "3584.1", "train_bsz": "144.8", "train_num_updates": "1101", "train_lr": "0.0027525", "train_gnorm": "0.584", "train_loss_scale": "64", "train_train_wall": "64", "train_gb_free": "38.9", "train_wall": "67"}
2023-12-13 23:32:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:32:04 | INFO | fairseq.trainer | begin training epoch 2
2023-12-13 23:32:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:32:11 | INFO | train_inner | {"epoch": 2, "update": 1.09, "loss": "6.777", "nll_loss": "5.78", "ppl": "54.97", "wps": "39924.1", "ups": "11.16", "wpb": "3578.3", "bsz": "130.7", "num_updates": "1200", "lr": "0.003", "gnorm": "0.502", "loss_scale": "64", "train_wall": "7", "gb_free": "38.9", "wall": "74"}
2023-12-13 23:32:18 | INFO | train_inner | {"epoch": 2, "update": 1.181, "loss": "6.564", "nll_loss": "5.528", "ppl": "46.14", "wps": "57817.6", "ups": "16.24", "wpb": "3561.2", "bsz": "135.6", "num_updates": "1300", "lr": "0.00325", "gnorm": "0.458", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "80"}
2023-12-13 23:32:23 | INFO | train_inner | {"epoch": 2, "update": 1.271, "loss": "6.262", "nll_loss": "5.171", "ppl": "36.04", "wps": "62463.2", "ups": "17.28", "wpb": "3614", "bsz": "161", "num_updates": "1400", "lr": "0.0035", "gnorm": "0.444", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "86"}
2023-12-13 23:32:29 | INFO | train_inner | {"epoch": 2, "update": 1.362, "loss": "6.152", "nll_loss": "5.041", "ppl": "32.93", "wps": "62488.5", "ups": "17.42", "wpb": "3587.2", "bsz": "154.9", "num_updates": "1500", "lr": "0.00375", "gnorm": "0.424", "loss_scale": "64", "train_wall": "6", "gb_free": "38.9", "wall": "92"}
2023-12-13 23:32:35 | INFO | train_inner | {"epoch": 2, "update": 1.453, "loss": "6.008", "nll_loss": "4.87", "ppl": "29.25", "wps": "65062", "ups": "17.89", "wpb": "3636.7", "bsz": "150.2", "num_updates": "1600", "lr": "0.004", "gnorm": "0.403", "loss_scale": "64", "train_wall": "5", "gb_free": "38.9", "wall": "97"}
2023-12-13 23:32:41 | INFO | train_inner | {"epoch": 2, "update": 1.544, "loss": "5.884", "nll_loss": "4.722", "ppl": "26.4", "wps": "60568.3", "ups": "16.97", "wpb": "3569.7", "bsz": "156.6", "num_updates": "1700", "lr": "0.00425", "gnorm": "0.405", "loss_scale": "64", "train_wall": "6", "gb_free": "39", "wall": "103"}
2023-12-13 23:32:46 | INFO | train_inner | {"epoch": 2, "update": 1.634, "loss": "5.828", "nll_loss": "4.66", "ppl": "25.28", "wps": "66935.9", "ups": "18.66", "wpb": "3588", "bsz": "162.5", "num_updates": "1800", "lr": "0.0045", "gnorm": "0.389", "loss_scale": "64", "train_wall": "5", "gb_free": "38.9", "wall": "109"}
2023-12-13 23:32:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-12-13 23:32:52 | INFO | train_inner | {"epoch": 2, "update": 1.726, "loss": "5.817", "nll_loss": "4.643", "ppl": "24.98", "wps": "55058.6", "ups": "15.35", "wpb": "3587.9", "bsz": "138.2", "num_updates": "1900", "lr": "0.00475", "gnorm": "0.391", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "115"}
2023-12-13 23:32:59 | INFO | train_inner | {"epoch": 2, "update": 1.817, "loss": "5.779", "nll_loss": "4.597", "ppl": "24.21", "wps": "57538.7", "ups": "16.13", "wpb": "3567.9", "bsz": "138.4", "num_updates": "2000", "lr": "0.005", "gnorm": "0.379", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "121"}
2023-12-13 23:33:05 | INFO | train_inner | {"epoch": 2, "update": 1.907, "loss": "5.733", "nll_loss": "4.544", "ppl": "23.33", "wps": "54811.8", "ups": "15.63", "wpb": "3506.2", "bsz": "136.3", "num_updates": "2100", "lr": "0.00525", "gnorm": "0.379", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "128"}
2023-12-13 23:33:11 | INFO | train_inner | {"epoch": 2, "update": 1.998, "loss": "5.664", "nll_loss": "4.467", "ppl": "22.11", "wps": "62184.3", "ups": "17.06", "wpb": "3644", "bsz": "135.4", "num_updates": "2200", "lr": "0.0055", "gnorm": "0.347", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "134"}
2023-12-13 23:33:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:33:13 | INFO | valid | {"epoch": 2, "valid_loss": "5.406", "valid_nll_loss": "4.206", "valid_ppl": "18.45", "valid_wps": "122602", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "2202", "valid_best_loss": "5.406"}
2023-12-13 23:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2202 updates
2023-12-13 23:33:13 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint2.pt
2023-12-13 23:33:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint2.pt
2023-12-13 23:33:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint2.pt (epoch 2 @ 2202 updates, score 5.406) (writing took 0.24419945012778044 seconds)
2023-12-13 23:33:13 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-12-13 23:33:13 | INFO | train | {"epoch": 2, "train_loss": "6.041", "train_nll_loss": "4.909", "train_ppl": "30.04", "train_wps": "57610.9", "train_ups": "16.07", "train_wpb": "3584.7", "train_bsz": "145.5", "train_num_updates": "2202", "train_lr": "0.005505", "train_gnorm": "0.411", "train_loss_scale": "32", "train_train_wall": "66", "train_gb_free": "38.9", "train_wall": "135"}
2023-12-13 23:33:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:33:13 | INFO | fairseq.trainer | begin training epoch 3
2023-12-13 23:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:33:19 | INFO | train_inner | {"epoch": 3, "update": 2.089, "loss": "5.539", "nll_loss": "4.317", "ppl": "19.93", "wps": "44388.3", "ups": "12.41", "wpb": "3577.6", "bsz": "137.2", "num_updates": "2300", "lr": "0.00575", "gnorm": "0.349", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "142"}
2023-12-13 23:33:25 | INFO | train_inner | {"epoch": 3, "update": 2.18, "loss": "5.517", "nll_loss": "4.291", "ppl": "19.57", "wps": "59549.9", "ups": "16.31", "wpb": "3650.4", "bsz": "140.9", "num_updates": "2400", "lr": "0.006", "gnorm": "0.341", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "148"}
2023-12-13 23:33:31 | INFO | train_inner | {"epoch": 3, "update": 2.27, "loss": "5.452", "nll_loss": "4.216", "ppl": "18.59", "wps": "61808.8", "ups": "17.24", "wpb": "3585.3", "bsz": "151.7", "num_updates": "2500", "lr": "0.00625", "gnorm": "0.34", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "154"}
2023-12-13 23:33:37 | INFO | train_inner | {"epoch": 3, "update": 2.361, "loss": "5.567", "nll_loss": "4.348", "ppl": "20.36", "wps": "53340.8", "ups": "15.25", "wpb": "3497.3", "bsz": "128.4", "num_updates": "2600", "lr": "0.0065", "gnorm": "0.342", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "160"}
2023-12-13 23:33:43 | INFO | train_inner | {"epoch": 3, "update": 2.452, "loss": "5.454", "nll_loss": "4.217", "ppl": "18.6", "wps": "59696.5", "ups": "16.68", "wpb": "3579.8", "bsz": "143.4", "num_updates": "2700", "lr": "0.00675", "gnorm": "0.322", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "166"}
2023-12-13 23:33:49 | INFO | train_inner | {"epoch": 3, "update": 2.543, "loss": "5.454", "nll_loss": "4.217", "ppl": "18.6", "wps": "59749.7", "ups": "16.82", "wpb": "3551.4", "bsz": "145.6", "num_updates": "2800", "lr": "0.007", "gnorm": "0.337", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "172"}
2023-12-13 23:33:55 | INFO | train_inner | {"epoch": 3, "update": 2.633, "loss": "5.4", "nll_loss": "4.156", "ppl": "17.83", "wps": "62894.6", "ups": "17.48", "wpb": "3598.9", "bsz": "156.9", "num_updates": "2900", "lr": "0.00725", "gnorm": "0.318", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "178"}
2023-12-13 23:34:01 | INFO | train_inner | {"epoch": 3, "update": 2.724, "loss": "5.444", "nll_loss": "4.205", "ppl": "18.44", "wps": "58964.8", "ups": "16.36", "wpb": "3604.9", "bsz": "140.5", "num_updates": "3000", "lr": "0.0075", "gnorm": "0.315", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "184"}
2023-12-13 23:34:07 | INFO | train_inner | {"epoch": 3, "update": 2.815, "loss": "5.383", "nll_loss": "4.135", "ppl": "17.57", "wps": "66630.2", "ups": "18.49", "wpb": "3603.8", "bsz": "155.7", "num_updates": "3100", "lr": "0.00775", "gnorm": "0.311", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "189"}
2023-12-13 23:34:12 | INFO | train_inner | {"epoch": 3, "update": 2.906, "loss": "5.407", "nll_loss": "4.162", "ppl": "17.9", "wps": "62939.6", "ups": "17.52", "wpb": "3591.8", "bsz": "156.6", "num_updates": "3200", "lr": "0.008", "gnorm": "0.309", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "195"}
2023-12-13 23:34:18 | INFO | train_inner | {"epoch": 3, "update": 2.996, "loss": "5.45", "nll_loss": "4.209", "ppl": "18.5", "wps": "60775.5", "ups": "17.01", "wpb": "3573.1", "bsz": "141.9", "num_updates": "3300", "lr": "0.00825", "gnorm": "0.31", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "201"}
2023-12-13 23:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:34:20 | INFO | valid | {"epoch": 3, "valid_loss": "5.145", "valid_nll_loss": "3.881", "valid_ppl": "14.73", "valid_wps": "128360", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "3304", "valid_best_loss": "5.145"}
2023-12-13 23:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3304 updates
2023-12-13 23:34:20 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint3.pt
2023-12-13 23:34:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint3.pt
2023-12-13 23:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint3.pt (epoch 3 @ 3304 updates, score 5.145) (writing took 0.2175622982904315 seconds)
2023-12-13 23:34:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-12-13 23:34:20 | INFO | train | {"epoch": 3, "train_loss": "5.46", "train_nll_loss": "4.224", "train_ppl": "18.69", "train_wps": "58692.9", "train_ups": "16.38", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "3304", "train_lr": "0.00826", "train_gnorm": "0.327", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "38.9", "train_wall": "203"}
2023-12-13 23:34:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:34:20 | INFO | fairseq.trainer | begin training epoch 4
2023-12-13 23:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:34:25 | INFO | train_inner | {"epoch": 4, "update": 3.087, "loss": "5.259", "nll_loss": "3.988", "ppl": "15.86", "wps": "49809.8", "ups": "13.94", "wpb": "3573.8", "bsz": "156.7", "num_updates": "3400", "lr": "0.0085", "gnorm": "0.3", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "208"}
2023-12-13 23:34:31 | INFO | train_inner | {"epoch": 4, "update": 3.178, "loss": "5.31", "nll_loss": "4.046", "ppl": "16.52", "wps": "62295.3", "ups": "17.43", "wpb": "3573.2", "bsz": "154.7", "num_updates": "3500", "lr": "0.00875", "gnorm": "0.304", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "214"}
2023-12-13 23:34:37 | INFO | train_inner | {"epoch": 4, "update": 3.269, "loss": "5.366", "nll_loss": "4.108", "ppl": "17.24", "wps": "59301.1", "ups": "16.48", "wpb": "3597.5", "bsz": "137.4", "num_updates": "3600", "lr": "0.009", "gnorm": "0.295", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "220"}
2023-12-13 23:34:43 | INFO | train_inner | {"epoch": 4, "update": 3.359, "loss": "5.411", "nll_loss": "4.159", "ppl": "17.86", "wps": "63193.4", "ups": "17.25", "wpb": "3662.4", "bsz": "140.2", "num_updates": "3700", "lr": "0.00925", "gnorm": "0.314", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "226"}
2023-12-13 23:34:49 | INFO | train_inner | {"epoch": 4, "update": 3.45, "loss": "5.373", "nll_loss": "4.115", "ppl": "17.33", "wps": "56393.8", "ups": "15.69", "wpb": "3594.6", "bsz": "144.2", "num_updates": "3800", "lr": "0.0095", "gnorm": "0.307", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "232"}
2023-12-13 23:34:56 | INFO | train_inner | {"epoch": 4, "update": 3.541, "loss": "5.389", "nll_loss": "4.127", "ppl": "17.47", "wps": "57700", "ups": "16.17", "wpb": "3567.9", "bsz": "148.4", "num_updates": "3900", "lr": "0.00975", "gnorm": "0.312", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "238"}
2023-12-13 23:35:02 | INFO | train_inner | {"epoch": 4, "update": 3.632, "loss": "5.405", "nll_loss": "4.149", "ppl": "17.74", "wps": "57677.1", "ups": "16.57", "wpb": "3481.3", "bsz": "143.9", "num_updates": "4000", "lr": "0.01", "gnorm": "0.32", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "244"}
2023-12-13 23:35:07 | INFO | train_inner | {"epoch": 4, "update": 3.722, "loss": "5.394", "nll_loss": "4.136", "ppl": "17.58", "wps": "62338.1", "ups": "17.35", "wpb": "3593.5", "bsz": "143", "num_updates": "4100", "lr": "0.0098773", "gnorm": "0.299", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "250"}
2023-12-13 23:35:14 | INFO | train_inner | {"epoch": 4, "update": 3.813, "loss": "5.473", "nll_loss": "4.227", "ppl": "18.72", "wps": "56464.9", "ups": "15.92", "wpb": "3546.3", "bsz": "126.6", "num_updates": "4200", "lr": "0.009759", "gnorm": "0.296", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "256"}
2023-12-13 23:35:19 | INFO | train_inner | {"epoch": 4, "update": 3.904, "loss": "5.301", "nll_loss": "4.026", "ppl": "16.29", "wps": "66120.7", "ups": "18.32", "wpb": "3610", "bsz": "159.8", "num_updates": "4300", "lr": "0.00964486", "gnorm": "0.302", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "262"}
2023-12-13 23:35:25 | INFO | train_inner | {"epoch": 4, "update": 3.995, "loss": "5.273", "nll_loss": "4", "ppl": "16", "wps": "63157.2", "ups": "17.44", "wpb": "3622.2", "bsz": "148.3", "num_updates": "4400", "lr": "0.00953463", "gnorm": "0.288", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "268"}
2023-12-13 23:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:35:27 | INFO | valid | {"epoch": 4, "valid_loss": "5.056", "valid_nll_loss": "3.785", "valid_ppl": "13.78", "valid_wps": "128553", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "4406", "valid_best_loss": "5.056"}
2023-12-13 23:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4406 updates
2023-12-13 23:35:27 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint4.pt
2023-12-13 23:35:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint4.pt
2023-12-13 23:35:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint4.pt (epoch 4 @ 4406 updates, score 5.056) (writing took 0.21519243624061346 seconds)
2023-12-13 23:35:27 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-12-13 23:35:27 | INFO | train | {"epoch": 4, "train_loss": "5.36", "train_nll_loss": "4.098", "train_ppl": "17.13", "train_wps": "59127", "train_ups": "16.5", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "4406", "train_lr": "0.00952813", "train_gnorm": "0.303", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "39", "train_wall": "270"}
2023-12-13 23:35:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:35:27 | INFO | fairseq.trainer | begin training epoch 5
2023-12-13 23:35:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:35:32 | INFO | train_inner | {"epoch": 5, "update": 4.085, "loss": "5.215", "nll_loss": "3.926", "ppl": "15.2", "wps": "48207.4", "ups": "13.34", "wpb": "3613.6", "bsz": "136.7", "num_updates": "4500", "lr": "0.00942809", "gnorm": "0.292", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "275"}
2023-12-13 23:35:38 | INFO | train_inner | {"epoch": 5, "update": 4.176, "loss": "5.191", "nll_loss": "3.905", "ppl": "14.98", "wps": "66072.8", "ups": "18.34", "wpb": "3601.8", "bsz": "155.9", "num_updates": "4600", "lr": "0.00932505", "gnorm": "0.286", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "280"}
2023-12-13 23:35:43 | INFO | train_inner | {"epoch": 5, "update": 4.267, "loss": "5.179", "nll_loss": "3.89", "ppl": "14.82", "wps": "66058.2", "ups": "18.29", "wpb": "3611.7", "bsz": "153.8", "num_updates": "4700", "lr": "0.00922531", "gnorm": "0.28", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "286"}
2023-12-13 23:35:49 | INFO | train_inner | {"epoch": 5, "update": 4.358, "loss": "5.269", "nll_loss": "3.993", "ppl": "15.92", "wps": "59338.3", "ups": "16.66", "wpb": "3562.5", "bsz": "133.8", "num_updates": "4800", "lr": "0.00912871", "gnorm": "0.299", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "292"}
2023-12-13 23:35:55 | INFO | train_inner | {"epoch": 5, "update": 4.448, "loss": "5.208", "nll_loss": "3.926", "ppl": "15.2", "wps": "59479.9", "ups": "16.92", "wpb": "3515.9", "bsz": "147.5", "num_updates": "4900", "lr": "0.00903508", "gnorm": "0.294", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "298"}
2023-12-13 23:36:01 | INFO | train_inner | {"epoch": 5, "update": 4.539, "loss": "5.195", "nll_loss": "3.91", "ppl": "15.04", "wps": "58093.4", "ups": "16.39", "wpb": "3545.3", "bsz": "140.5", "num_updates": "5000", "lr": "0.00894427", "gnorm": "0.282", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "304"}
2023-12-13 23:36:07 | INFO | train_inner | {"epoch": 5, "update": 4.63, "loss": "5.137", "nll_loss": "3.847", "ppl": "14.39", "wps": "63099.4", "ups": "17.54", "wpb": "3596.4", "bsz": "151.8", "num_updates": "5100", "lr": "0.00885615", "gnorm": "0.278", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "310"}
2023-12-13 23:36:13 | INFO | train_inner | {"epoch": 5, "update": 4.721, "loss": "5.216", "nll_loss": "3.936", "ppl": "15.3", "wps": "61253.6", "ups": "17.09", "wpb": "3584.9", "bsz": "141.2", "num_updates": "5200", "lr": "0.00877058", "gnorm": "0.299", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "315"}
2023-12-13 23:36:19 | INFO | train_inner | {"epoch": 5, "update": 4.811, "loss": "5.147", "nll_loss": "3.859", "ppl": "14.51", "wps": "57931.1", "ups": "16.21", "wpb": "3574.1", "bsz": "147", "num_updates": "5300", "lr": "0.00868744", "gnorm": "0.276", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "322"}
2023-12-13 23:36:25 | INFO | train_inner | {"epoch": 5, "update": 4.902, "loss": "5.114", "nll_loss": "3.825", "ppl": "14.17", "wps": "63526.8", "ups": "17.62", "wpb": "3605.6", "bsz": "152.4", "num_updates": "5400", "lr": "0.00860663", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "327"}
2023-12-13 23:36:31 | INFO | train_inner | {"epoch": 5, "update": 4.993, "loss": "5.21", "nll_loss": "3.933", "ppl": "15.27", "wps": "54752.2", "ups": "15.27", "wpb": "3586.3", "bsz": "132.2", "num_updates": "5500", "lr": "0.00852803", "gnorm": "0.284", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "334"}
2023-12-13 23:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:36:33 | INFO | valid | {"epoch": 5, "valid_loss": "4.93", "valid_nll_loss": "3.629", "valid_ppl": "12.37", "valid_wps": "126926", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "5508", "valid_best_loss": "4.93"}
2023-12-13 23:36:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5508 updates
2023-12-13 23:36:33 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint5.pt
2023-12-13 23:36:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint5.pt
2023-12-13 23:36:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint5.pt (epoch 5 @ 5508 updates, score 4.93) (writing took 0.2160156099125743 seconds)
2023-12-13 23:36:33 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-12-13 23:36:33 | INFO | train | {"epoch": 5, "train_loss": "5.186", "train_nll_loss": "3.901", "train_ppl": "14.94", "train_wps": "59472.9", "train_ups": "16.6", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "5508", "train_lr": "0.00852183", "train_gnorm": "0.285", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "38.9", "train_wall": "336"}
2023-12-13 23:36:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:36:33 | INFO | fairseq.trainer | begin training epoch 6
2023-12-13 23:36:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:36:39 | INFO | train_inner | {"epoch": 6, "update": 5.083, "loss": "5.022", "nll_loss": "3.714", "ppl": "13.12", "wps": "47235.5", "ups": "13.27", "wpb": "3560.2", "bsz": "142.3", "num_updates": "5600", "lr": "0.00845154", "gnorm": "0.269", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "341"}
2023-12-13 23:36:44 | INFO | train_inner | {"epoch": 6, "update": 5.174, "loss": "5.014", "nll_loss": "3.706", "ppl": "13.05", "wps": "66457.8", "ups": "18.28", "wpb": "3636.4", "bsz": "147.4", "num_updates": "5700", "lr": "0.00837708", "gnorm": "0.268", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "347"}
2023-12-13 23:36:50 | INFO | train_inner | {"epoch": 6, "update": 5.265, "loss": "5.023", "nll_loss": "3.715", "ppl": "13.13", "wps": "62799.2", "ups": "17.67", "wpb": "3554.8", "bsz": "147.2", "num_updates": "5800", "lr": "0.00830455", "gnorm": "0.273", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "353"}
2023-12-13 23:36:55 | INFO | train_inner | {"epoch": 6, "update": 5.356, "loss": "4.945", "nll_loss": "3.629", "ppl": "12.37", "wps": "68362.8", "ups": "18.6", "wpb": "3675", "bsz": "157.7", "num_updates": "5900", "lr": "0.00823387", "gnorm": "0.26", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "358"}
2023-12-13 23:37:01 | INFO | train_inner | {"epoch": 6, "update": 5.446, "loss": "5.078", "nll_loss": "3.782", "ppl": "13.76", "wps": "57143.5", "ups": "16.36", "wpb": "3493", "bsz": "139.2", "num_updates": "6000", "lr": "0.00816497", "gnorm": "0.283", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "364"}
2023-12-13 23:37:08 | INFO | train_inner | {"epoch": 6, "update": 5.537, "loss": "5.113", "nll_loss": "3.823", "ppl": "14.15", "wps": "55727.5", "ups": "15.9", "wpb": "3504.5", "bsz": "127.1", "num_updates": "6100", "lr": "0.00809776", "gnorm": "0.269", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "370"}
2023-12-13 23:37:13 | INFO | train_inner | {"epoch": 6, "update": 5.628, "loss": "4.97", "nll_loss": "3.658", "ppl": "12.62", "wps": "66887.9", "ups": "18.44", "wpb": "3627.1", "bsz": "163", "num_updates": "6200", "lr": "0.00803219", "gnorm": "0.285", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "376"}
2023-12-13 23:37:18 | INFO | train_inner | {"epoch": 6, "update": 5.719, "loss": "5.015", "nll_loss": "3.713", "ppl": "13.11", "wps": "66006.9", "ups": "18.4", "wpb": "3587.4", "bsz": "145.7", "num_updates": "6300", "lr": "0.00796819", "gnorm": "0.263", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "381"}
2023-12-13 23:37:25 | INFO | train_inner | {"epoch": 6, "update": 5.809, "loss": "5.117", "nll_loss": "3.83", "ppl": "14.22", "wps": "55405.4", "ups": "15.5", "wpb": "3573.6", "bsz": "131.4", "num_updates": "6400", "lr": "0.00790569", "gnorm": "0.284", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "388"}
2023-12-13 23:37:32 | INFO | train_inner | {"epoch": 6, "update": 5.9, "loss": "5.028", "nll_loss": "3.725", "ppl": "13.23", "wps": "54110.1", "ups": "14.97", "wpb": "3613.4", "bsz": "130.9", "num_updates": "6500", "lr": "0.00784465", "gnorm": "0.269", "loss_scale": "32", "train_wall": "7", "gb_free": "38.9", "wall": "394"}
2023-12-13 23:37:37 | INFO | train_inner | {"epoch": 6, "update": 5.991, "loss": "4.917", "nll_loss": "3.601", "ppl": "12.13", "wps": "67737.2", "ups": "18.79", "wpb": "3604.3", "bsz": "166.5", "num_updates": "6600", "lr": "0.00778499", "gnorm": "0.267", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "400"}
2023-12-13 23:37:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:37:39 | INFO | valid | {"epoch": 6, "valid_loss": "4.836", "valid_nll_loss": "3.532", "valid_ppl": "11.57", "valid_wps": "129656", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "6610", "valid_best_loss": "4.836"}
2023-12-13 23:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6610 updates
2023-12-13 23:37:39 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint6.pt
2023-12-13 23:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint6.pt
2023-12-13 23:37:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint6.pt (epoch 6 @ 6610 updates, score 4.836) (writing took 0.21247121039777994 seconds)
2023-12-13 23:37:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-12-13 23:37:39 | INFO | train | {"epoch": 6, "train_loss": "5.02", "train_nll_loss": "3.716", "train_ppl": "13.14", "train_wps": "59942.7", "train_ups": "16.73", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "6610", "train_lr": "0.0077791", "train_gnorm": "0.272", "train_loss_scale": "32", "train_train_wall": "63", "train_gb_free": "38.9", "train_wall": "402"}
2023-12-13 23:37:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:37:39 | INFO | fairseq.trainer | begin training epoch 7
2023-12-13 23:37:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:37:44 | INFO | train_inner | {"epoch": 7, "update": 6.082, "loss": "4.881", "nll_loss": "3.556", "ppl": "11.77", "wps": "49055.2", "ups": "13.77", "wpb": "3563", "bsz": "152.2", "num_updates": "6700", "lr": "0.00772667", "gnorm": "0.27", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "407"}
2023-12-13 23:37:50 | INFO | train_inner | {"epoch": 7, "update": 6.172, "loss": "4.867", "nll_loss": "3.545", "ppl": "11.67", "wps": "66888", "ups": "18.74", "wpb": "3569.3", "bsz": "156.7", "num_updates": "6800", "lr": "0.00766965", "gnorm": "0.26", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "412"}
2023-12-13 23:37:55 | INFO | train_inner | {"epoch": 7, "update": 6.263, "loss": "4.913", "nll_loss": "3.595", "ppl": "12.09", "wps": "67172", "ups": "18.56", "wpb": "3618.6", "bsz": "135.5", "num_updates": "6900", "lr": "0.00761387", "gnorm": "0.253", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "418"}
2023-12-13 23:38:01 | INFO | train_inner | {"epoch": 7, "update": 6.354, "loss": "4.937", "nll_loss": "3.622", "ppl": "12.31", "wps": "57912.5", "ups": "16.49", "wpb": "3511.9", "bsz": "141.3", "num_updates": "7000", "lr": "0.00755929", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "424"}
2023-12-13 23:38:06 | INFO | train_inner | {"epoch": 7, "update": 6.445, "loss": "4.841", "nll_loss": "3.514", "ppl": "11.43", "wps": "67453.7", "ups": "18.67", "wpb": "3612.9", "bsz": "159.4", "num_updates": "7100", "lr": "0.00750587", "gnorm": "0.25", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "429"}
2023-12-13 23:38:12 | INFO | train_inner | {"epoch": 7, "update": 6.535, "loss": "4.929", "nll_loss": "3.616", "ppl": "12.26", "wps": "62039.2", "ups": "17.05", "wpb": "3638", "bsz": "133.5", "num_updates": "7200", "lr": "0.00745356", "gnorm": "0.262", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "435"}
2023-12-13 23:38:18 | INFO | train_inner | {"epoch": 7, "update": 6.626, "loss": "4.855", "nll_loss": "3.531", "ppl": "11.56", "wps": "60819.5", "ups": "16.88", "wpb": "3602.1", "bsz": "164.2", "num_updates": "7300", "lr": "0.00740233", "gnorm": "0.27", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "441"}
2023-12-13 23:38:25 | INFO | train_inner | {"epoch": 7, "update": 6.717, "loss": "5.006", "nll_loss": "3.704", "ppl": "13.03", "wps": "52202.1", "ups": "14.8", "wpb": "3526.8", "bsz": "134.8", "num_updates": "7400", "lr": "0.00735215", "gnorm": "0.271", "loss_scale": "32", "train_wall": "7", "gb_free": "38.9", "wall": "448"}
2023-12-13 23:38:31 | INFO | train_inner | {"epoch": 7, "update": 6.808, "loss": "4.895", "nll_loss": "3.579", "ppl": "11.95", "wps": "60483.3", "ups": "16.85", "wpb": "3590.2", "bsz": "143.8", "num_updates": "7500", "lr": "0.00730297", "gnorm": "0.257", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "454"}
2023-12-13 23:38:37 | INFO | train_inner | {"epoch": 7, "update": 6.898, "loss": "4.912", "nll_loss": "3.6", "ppl": "12.12", "wps": "59732.3", "ups": "16.7", "wpb": "3576", "bsz": "152.7", "num_updates": "7600", "lr": "0.00725476", "gnorm": "0.261", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "460"}
2023-12-13 23:38:43 | INFO | train_inner | {"epoch": 7, "update": 6.989, "loss": "4.943", "nll_loss": "3.633", "ppl": "12.41", "wps": "56877.5", "ups": "15.68", "wpb": "3627.3", "bsz": "127.8", "num_updates": "7700", "lr": "0.0072075", "gnorm": "0.277", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "466"}
2023-12-13 23:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:38:45 | INFO | valid | {"epoch": 7, "valid_loss": "4.763", "valid_nll_loss": "3.472", "valid_ppl": "11.1", "valid_wps": "128364", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "7712", "valid_best_loss": "4.763"}
2023-12-13 23:38:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7712 updates
2023-12-13 23:38:45 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint7.pt
2023-12-13 23:38:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint7.pt
2023-12-13 23:38:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint7.pt (epoch 7 @ 7712 updates, score 4.763) (writing took 0.21552042849361897 seconds)
2023-12-13 23:38:46 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-12-13 23:38:46 | INFO | train | {"epoch": 7, "train_loss": "4.906", "train_nll_loss": "3.59", "train_ppl": "12.04", "train_wps": "59469.7", "train_ups": "16.6", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "7712", "train_lr": "0.00720189", "train_gnorm": "0.264", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "39", "train_wall": "468"}
2023-12-13 23:38:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:38:46 | INFO | fairseq.trainer | begin training epoch 8
2023-12-13 23:38:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:38:51 | INFO | train_inner | {"epoch": 8, "update": 7.08, "loss": "4.742", "nll_loss": "3.399", "ppl": "10.55", "wps": "44034.7", "ups": "12.47", "wpb": "3530.2", "bsz": "150.7", "num_updates": "7800", "lr": "0.00716115", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "474"}
2023-12-13 23:38:56 | INFO | train_inner | {"epoch": 8, "update": 7.171, "loss": "4.703", "nll_loss": "3.357", "ppl": "10.24", "wps": "69924.5", "ups": "19.07", "wpb": "3666.6", "bsz": "170.1", "num_updates": "7900", "lr": "0.00711568", "gnorm": "0.251", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "479"}
2023-12-13 23:39:02 | INFO | train_inner | {"epoch": 8, "update": 7.261, "loss": "4.886", "nll_loss": "3.567", "ppl": "11.85", "wps": "58872.4", "ups": "16.74", "wpb": "3517.6", "bsz": "133", "num_updates": "8000", "lr": "0.00707107", "gnorm": "0.273", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "485"}
2023-12-13 23:39:09 | INFO | train_inner | {"epoch": 8, "update": 7.352, "loss": "4.85", "nll_loss": "3.525", "ppl": "11.51", "wps": "56352.4", "ups": "15.74", "wpb": "3579.7", "bsz": "133.1", "num_updates": "8100", "lr": "0.00702728", "gnorm": "0.26", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "491"}
2023-12-13 23:39:15 | INFO | train_inner | {"epoch": 8, "update": 7.443, "loss": "4.824", "nll_loss": "3.497", "ppl": "11.29", "wps": "62074.8", "ups": "17.46", "wpb": "3554.8", "bsz": "144.2", "num_updates": "8200", "lr": "0.0069843", "gnorm": "0.262", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "497"}
2023-12-13 23:39:20 | INFO | train_inner | {"epoch": 8, "update": 7.534, "loss": "4.844", "nll_loss": "3.522", "ppl": "11.49", "wps": "65342.3", "ups": "18.07", "wpb": "3616.3", "bsz": "145.6", "num_updates": "8300", "lr": "0.0069421", "gnorm": "0.254", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "503"}
2023-12-13 23:39:26 | INFO | train_inner | {"epoch": 8, "update": 7.624, "loss": "4.816", "nll_loss": "3.489", "ppl": "11.23", "wps": "62225.8", "ups": "17.53", "wpb": "3550.1", "bsz": "141.7", "num_updates": "8400", "lr": "0.00690066", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "508"}
2023-12-13 23:39:32 | INFO | train_inner | {"epoch": 8, "update": 7.715, "loss": "4.811", "nll_loss": "3.485", "ppl": "11.19", "wps": "60727.3", "ups": "16.89", "wpb": "3595.9", "bsz": "150.7", "num_updates": "8500", "lr": "0.00685994", "gnorm": "0.251", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "514"}
2023-12-13 23:39:37 | INFO | train_inner | {"epoch": 8, "update": 7.806, "loss": "4.811", "nll_loss": "3.484", "ppl": "11.19", "wps": "62720.3", "ups": "17.41", "wpb": "3603.1", "bsz": "149.3", "num_updates": "8600", "lr": "0.00681994", "gnorm": "0.269", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "520"}
2023-12-13 23:39:43 | INFO | train_inner | {"epoch": 8, "update": 7.897, "loss": "4.825", "nll_loss": "3.502", "ppl": "11.33", "wps": "63225.9", "ups": "17.49", "wpb": "3615", "bsz": "144.8", "num_updates": "8700", "lr": "0.00678064", "gnorm": "0.256", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "526"}
2023-12-13 23:39:49 | INFO | train_inner | {"epoch": 8, "update": 7.987, "loss": "4.843", "nll_loss": "3.522", "ppl": "11.49", "wps": "57956.1", "ups": "16.11", "wpb": "3597.5", "bsz": "140.8", "num_updates": "8800", "lr": "0.006742", "gnorm": "0.271", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "532"}
2023-12-13 23:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:39:52 | INFO | valid | {"epoch": 8, "valid_loss": "4.675", "valid_nll_loss": "3.378", "valid_ppl": "10.4", "valid_wps": "128599", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "8814", "valid_best_loss": "4.675"}
2023-12-13 23:39:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8814 updates
2023-12-13 23:39:52 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint8.pt
2023-12-13 23:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint8.pt
2023-12-13 23:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint8.pt (epoch 8 @ 8814 updates, score 4.675) (writing took 0.21860313974320889 seconds)
2023-12-13 23:39:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-12-13 23:39:52 | INFO | train | {"epoch": 8, "train_loss": "4.816", "train_nll_loss": "3.488", "train_ppl": "11.22", "train_wps": "59527.8", "train_ups": "16.61", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "8814", "train_lr": "0.00673664", "train_gnorm": "0.261", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "38.9", "train_wall": "535"}
2023-12-13 23:39:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:39:52 | INFO | fairseq.trainer | begin training epoch 9
2023-12-13 23:39:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:39:57 | INFO | train_inner | {"epoch": 9, "update": 8.078, "loss": "4.705", "nll_loss": "3.36", "ppl": "10.27", "wps": "47799.9", "ups": "13.32", "wpb": "3589.5", "bsz": "145.1", "num_updates": "8900", "lr": "0.00670402", "gnorm": "0.251", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "540"}
2023-12-13 23:40:03 | INFO | train_inner | {"epoch": 9, "update": 8.169, "loss": "4.693", "nll_loss": "3.346", "ppl": "10.17", "wps": "58962.6", "ups": "16.42", "wpb": "3591.1", "bsz": "147.7", "num_updates": "9000", "lr": "0.00666667", "gnorm": "0.258", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "546"}
2023-12-13 23:40:08 | INFO | train_inner | {"epoch": 9, "update": 8.26, "loss": "4.709", "nll_loss": "3.367", "ppl": "10.32", "wps": "68583", "ups": "18.95", "wpb": "3618.3", "bsz": "156.6", "num_updates": "9100", "lr": "0.00662994", "gnorm": "0.258", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "551"}
2023-12-13 23:40:14 | INFO | train_inner | {"epoch": 9, "update": 8.35, "loss": "4.704", "nll_loss": "3.36", "ppl": "10.27", "wps": "66674.8", "ups": "18.48", "wpb": "3607.8", "bsz": "153.8", "num_updates": "9200", "lr": "0.0065938", "gnorm": "0.255", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "556"}
2023-12-13 23:40:20 | INFO | train_inner | {"epoch": 9, "update": 8.441, "loss": "4.823", "nll_loss": "3.497", "ppl": "11.29", "wps": "56016.8", "ups": "15.85", "wpb": "3535", "bsz": "130.7", "num_updates": "9300", "lr": "0.00655826", "gnorm": "0.287", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "563"}
2023-12-13 23:40:26 | INFO | train_inner | {"epoch": 9, "update": 8.532, "loss": "4.755", "nll_loss": "3.421", "ppl": "10.71", "wps": "60736.8", "ups": "17.08", "wpb": "3556.3", "bsz": "144", "num_updates": "9400", "lr": "0.00652328", "gnorm": "0.249", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "569"}
2023-12-13 23:40:32 | INFO | train_inner | {"epoch": 9, "update": 8.623, "loss": "4.792", "nll_loss": "3.463", "ppl": "11.03", "wps": "60488.8", "ups": "16.84", "wpb": "3592", "bsz": "132.8", "num_updates": "9500", "lr": "0.00648886", "gnorm": "0.257", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "574"}
2023-12-13 23:40:38 | INFO | train_inner | {"epoch": 9, "update": 8.713, "loss": "4.777", "nll_loss": "3.448", "ppl": "10.92", "wps": "59673.5", "ups": "16.85", "wpb": "3541.2", "bsz": "149.2", "num_updates": "9600", "lr": "0.00645497", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "580"}
2023-12-13 23:40:44 | INFO | train_inner | {"epoch": 9, "update": 8.804, "loss": "4.797", "nll_loss": "3.47", "ppl": "11.08", "wps": "56547.8", "ups": "16.04", "wpb": "3526.3", "bsz": "146.5", "num_updates": "9700", "lr": "0.00642161", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "587"}
2023-12-13 23:40:49 | INFO | train_inner | {"epoch": 9, "update": 8.895, "loss": "4.752", "nll_loss": "3.42", "ppl": "10.71", "wps": "65549.5", "ups": "18.11", "wpb": "3619", "bsz": "149.5", "num_updates": "9800", "lr": "0.00638877", "gnorm": "0.261", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "592"}
2023-12-13 23:40:56 | INFO | train_inner | {"epoch": 9, "update": 8.985, "loss": "4.795", "nll_loss": "3.469", "ppl": "11.07", "wps": "57828.8", "ups": "15.91", "wpb": "3634.2", "bsz": "144", "num_updates": "9900", "lr": "0.00635642", "gnorm": "0.255", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "598"}
2023-12-13 23:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:40:58 | INFO | valid | {"epoch": 9, "valid_loss": "4.59", "valid_nll_loss": "3.269", "valid_ppl": "9.64", "valid_wps": "128614", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "9916", "valid_best_loss": "4.59"}
2023-12-13 23:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9916 updates
2023-12-13 23:40:58 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint9.pt
2023-12-13 23:40:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint9.pt
2023-12-13 23:40:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint9.pt (epoch 9 @ 9916 updates, score 4.59) (writing took 0.21626533567905426 seconds)
2023-12-13 23:40:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-12-13 23:40:58 | INFO | train | {"epoch": 9, "train_loss": "4.753", "train_nll_loss": "3.418", "train_ppl": "10.69", "train_wps": "59436.1", "train_ups": "16.59", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "9916", "train_lr": "0.00635129", "train_gnorm": "0.259", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "38.9", "train_wall": "601"}
2023-12-13 23:40:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:40:58 | INFO | fairseq.trainer | begin training epoch 10
2023-12-13 23:40:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:41:03 | INFO | train_inner | {"epoch": 10, "update": 9.076, "loss": "4.655", "nll_loss": "3.305", "ppl": "9.89", "wps": "47521.7", "ups": "13.39", "wpb": "3548.8", "bsz": "138.9", "num_updates": "10000", "lr": "0.00632456", "gnorm": "0.244", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "606"}
2023-12-13 23:41:09 | INFO | train_inner | {"epoch": 10, "update": 9.167, "loss": "4.571", "nll_loss": "3.209", "ppl": "9.25", "wps": "65732.4", "ups": "18.22", "wpb": "3607.9", "bsz": "164", "num_updates": "10100", "lr": "0.00629317", "gnorm": "0.25", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "611"}
2023-12-13 23:41:15 | INFO | train_inner | {"epoch": 10, "update": 9.258, "loss": "4.705", "nll_loss": "3.362", "ppl": "10.28", "wps": "55543.1", "ups": "15.61", "wpb": "3559", "bsz": "134.1", "num_updates": "10200", "lr": "0.00626224", "gnorm": "0.258", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "618"}
2023-12-13 23:41:21 | INFO | train_inner | {"epoch": 10, "update": 9.348, "loss": "4.707", "nll_loss": "3.363", "ppl": "10.29", "wps": "60348.4", "ups": "17.01", "wpb": "3548.7", "bsz": "145", "num_updates": "10300", "lr": "0.00623177", "gnorm": "0.278", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "624"}
2023-12-13 23:41:27 | INFO | train_inner | {"epoch": 10, "update": 9.439, "loss": "4.641", "nll_loss": "3.292", "ppl": "9.79", "wps": "63933.3", "ups": "17.9", "wpb": "3571.7", "bsz": "167.8", "num_updates": "10400", "lr": "0.00620174", "gnorm": "0.256", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "629"}
2023-12-13 23:41:32 | INFO | train_inner | {"epoch": 10, "update": 9.53, "loss": "4.654", "nll_loss": "3.309", "ppl": "9.91", "wps": "66058.4", "ups": "18.47", "wpb": "3575.7", "bsz": "158.8", "num_updates": "10500", "lr": "0.00617213", "gnorm": "0.251", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "635"}
2023-12-13 23:41:38 | INFO | train_inner | {"epoch": 10, "update": 9.621, "loss": "4.745", "nll_loss": "3.411", "ppl": "10.63", "wps": "56940.5", "ups": "15.8", "wpb": "3603.7", "bsz": "132.8", "num_updates": "10600", "lr": "0.00614295", "gnorm": "0.256", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "641"}
2023-12-13 23:41:45 | INFO | train_inner | {"epoch": 10, "update": 9.711, "loss": "4.772", "nll_loss": "3.442", "ppl": "10.86", "wps": "54009.1", "ups": "15.48", "wpb": "3488.6", "bsz": "132.9", "num_updates": "10700", "lr": "0.00611418", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "647"}
2023-12-13 23:41:50 | INFO | train_inner | {"epoch": 10, "update": 9.802, "loss": "4.697", "nll_loss": "3.359", "ppl": "10.26", "wps": "65139.7", "ups": "17.82", "wpb": "3656.3", "bsz": "142.3", "num_updates": "10800", "lr": "0.00608581", "gnorm": "0.237", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "653"}
2023-12-13 23:41:56 | INFO | train_inner | {"epoch": 10, "update": 9.893, "loss": "4.741", "nll_loss": "3.409", "ppl": "10.62", "wps": "62068.8", "ups": "17.08", "wpb": "3633.5", "bsz": "132.8", "num_updates": "10900", "lr": "0.00605783", "gnorm": "0.247", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "659"}
2023-12-13 23:42:01 | INFO | train_inner | {"epoch": 10, "update": 9.984, "loss": "4.712", "nll_loss": "3.377", "ppl": "10.39", "wps": "70198.6", "ups": "19.41", "wpb": "3616.5", "bsz": "152.6", "num_updates": "11000", "lr": "0.00603023", "gnorm": "0.255", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "664"}
2023-12-13 23:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:42:04 | INFO | valid | {"epoch": 10, "valid_loss": "4.565", "valid_nll_loss": "3.224", "valid_ppl": "9.34", "valid_wps": "129086", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "11018", "valid_best_loss": "4.565"}
2023-12-13 23:42:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 11018 updates
2023-12-13 23:42:04 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint10.pt
2023-12-13 23:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint10.pt
2023-12-13 23:42:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint10.pt (epoch 10 @ 11018 updates, score 4.565) (writing took 0.21391832642257214 seconds)
2023-12-13 23:42:04 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-12-13 23:42:04 | INFO | train | {"epoch": 10, "train_loss": "4.691", "train_nll_loss": "3.349", "train_ppl": "10.19", "train_wps": "60075.5", "train_ups": "16.76", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "11018", "train_lr": "0.0060253", "train_gnorm": "0.255", "train_loss_scale": "32", "train_train_wall": "63", "train_gb_free": "38.9", "train_wall": "667"}
2023-12-13 23:42:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:42:04 | INFO | fairseq.trainer | begin training epoch 11
2023-12-13 23:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:42:09 | INFO | train_inner | {"epoch": 11, "update": 10.074, "loss": "4.601", "nll_loss": "3.244", "ppl": "9.48", "wps": "47047.6", "ups": "13.14", "wpb": "3580.1", "bsz": "143.8", "num_updates": "11100", "lr": "0.006003", "gnorm": "0.254", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "672"}
2023-12-13 23:42:14 | INFO | train_inner | {"epoch": 11, "update": 10.165, "loss": "4.602", "nll_loss": "3.246", "ppl": "9.49", "wps": "65197.2", "ups": "18.31", "wpb": "3561.3", "bsz": "148.4", "num_updates": "11200", "lr": "0.00597614", "gnorm": "0.251", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "677"}
2023-12-13 23:42:21 | INFO | train_inner | {"epoch": 11, "update": 10.256, "loss": "4.596", "nll_loss": "3.239", "ppl": "9.44", "wps": "56548.6", "ups": "15.81", "wpb": "3576.6", "bsz": "148.5", "num_updates": "11300", "lr": "0.00594964", "gnorm": "0.255", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "683"}
2023-12-13 23:42:27 | INFO | train_inner | {"epoch": 11, "update": 10.347, "loss": "4.615", "nll_loss": "3.263", "ppl": "9.6", "wps": "61128.4", "ups": "16.94", "wpb": "3607.7", "bsz": "141.4", "num_updates": "11400", "lr": "0.00592349", "gnorm": "0.241", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "689"}
2023-12-13 23:42:33 | INFO | train_inner | {"epoch": 11, "update": 10.437, "loss": "4.739", "nll_loss": "3.405", "ppl": "10.59", "wps": "57368.9", "ups": "16.25", "wpb": "3529.6", "bsz": "129.8", "num_updates": "11500", "lr": "0.00589768", "gnorm": "0.27", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "696"}
2023-12-13 23:42:39 | INFO | train_inner | {"epoch": 11, "update": 10.528, "loss": "4.631", "nll_loss": "3.282", "ppl": "9.73", "wps": "60174.6", "ups": "16.96", "wpb": "3547", "bsz": "148.9", "num_updates": "11600", "lr": "0.0058722", "gnorm": "0.248", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "701"}
2023-12-13 23:42:44 | INFO | train_inner | {"epoch": 11, "update": 10.619, "loss": "4.602", "nll_loss": "3.249", "ppl": "9.5", "wps": "65252.4", "ups": "17.73", "wpb": "3681.1", "bsz": "158.1", "num_updates": "11700", "lr": "0.00584705", "gnorm": "0.249", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "707"}
2023-12-13 23:42:51 | INFO | train_inner | {"epoch": 11, "update": 10.71, "loss": "4.723", "nll_loss": "3.387", "ppl": "10.46", "wps": "56940.3", "ups": "16.21", "wpb": "3511.7", "bsz": "136.9", "num_updates": "11800", "lr": "0.00582223", "gnorm": "0.297", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "713"}
2023-12-13 23:42:56 | INFO | train_inner | {"epoch": 11, "update": 10.8, "loss": "4.661", "nll_loss": "3.318", "ppl": "9.97", "wps": "60332.1", "ups": "16.79", "wpb": "3593.1", "bsz": "150.3", "num_updates": "11900", "lr": "0.00579771", "gnorm": "0.251", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "719"}
2023-12-13 23:43:02 | INFO | train_inner | {"epoch": 11, "update": 10.891, "loss": "4.606", "nll_loss": "3.258", "ppl": "9.57", "wps": "69867.5", "ups": "19.25", "wpb": "3628.9", "bsz": "155.6", "num_updates": "12000", "lr": "0.0057735", "gnorm": "0.241", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "724"}
2023-12-13 23:43:07 | INFO | train_inner | {"epoch": 11, "update": 10.982, "loss": "4.694", "nll_loss": "3.356", "ppl": "10.24", "wps": "63660.7", "ups": "17.64", "wpb": "3609.6", "bsz": "135.3", "num_updates": "12100", "lr": "0.0057496", "gnorm": "0.256", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "730"}
2023-12-13 23:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:43:10 | INFO | valid | {"epoch": 11, "valid_loss": "4.554", "valid_nll_loss": "3.213", "valid_ppl": "9.27", "valid_wps": "127402", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "12120", "valid_best_loss": "4.554"}
2023-12-13 23:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 12120 updates
2023-12-13 23:43:10 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint11.pt
2023-12-13 23:43:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint11.pt
2023-12-13 23:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint11.pt (epoch 11 @ 12120 updates, score 4.554) (writing took 0.22492920700460672 seconds)
2023-12-13 23:43:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-12-13 23:43:10 | INFO | train | {"epoch": 11, "train_loss": "4.641", "train_nll_loss": "3.293", "train_ppl": "9.8", "train_wps": "59751.6", "train_ups": "16.67", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "12120", "train_lr": "0.00574485", "train_gnorm": "0.256", "train_loss_scale": "32", "train_train_wall": "63", "train_gb_free": "38.9", "train_wall": "733"}
2023-12-13 23:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:43:10 | INFO | fairseq.trainer | begin training epoch 12
2023-12-13 23:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:43:15 | INFO | train_inner | {"epoch": 12, "update": 11.073, "loss": "4.548", "nll_loss": "3.186", "ppl": "9.1", "wps": "47069.2", "ups": "13.11", "wpb": "3591.2", "bsz": "149.9", "num_updates": "12200", "lr": "0.00572598", "gnorm": "0.248", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "738"}
2023-12-13 23:43:21 | INFO | train_inner | {"epoch": 12, "update": 11.163, "loss": "4.54", "nll_loss": "3.175", "ppl": "9.03", "wps": "57915.2", "ups": "15.98", "wpb": "3625.2", "bsz": "143", "num_updates": "12300", "lr": "0.00570266", "gnorm": "0.253", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "744"}
2023-12-13 23:43:27 | INFO | train_inner | {"epoch": 12, "update": 11.254, "loss": "4.574", "nll_loss": "3.215", "ppl": "9.28", "wps": "64073.6", "ups": "17.77", "wpb": "3605.4", "bsz": "142.9", "num_updates": "12400", "lr": "0.00567962", "gnorm": "0.258", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "750"}
2023-12-13 23:43:33 | INFO | train_inner | {"epoch": 12, "update": 11.345, "loss": "4.604", "nll_loss": "3.25", "ppl": "9.52", "wps": "59406.3", "ups": "16.62", "wpb": "3575.4", "bsz": "145.4", "num_updates": "12500", "lr": "0.00565685", "gnorm": "0.27", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "756"}
2023-12-13 23:43:39 | INFO | train_inner | {"epoch": 12, "update": 11.436, "loss": "4.61", "nll_loss": "3.257", "ppl": "9.56", "wps": "58617.2", "ups": "16.57", "wpb": "3537.8", "bsz": "144.2", "num_updates": "12600", "lr": "0.00563436", "gnorm": "0.254", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "762"}
2023-12-13 23:43:45 | INFO | train_inner | {"epoch": 12, "update": 11.526, "loss": "4.599", "nll_loss": "3.246", "ppl": "9.49", "wps": "61031.5", "ups": "16.83", "wpb": "3627", "bsz": "139.9", "num_updates": "12700", "lr": "0.00561214", "gnorm": "0.245", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "768"}
2023-12-13 23:43:51 | INFO | train_inner | {"epoch": 12, "update": 11.617, "loss": "4.572", "nll_loss": "3.215", "ppl": "9.29", "wps": "61955.8", "ups": "17.28", "wpb": "3585.1", "bsz": "157.6", "num_updates": "12800", "lr": "0.00559017", "gnorm": "0.25", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "773"}
2023-12-13 23:43:56 | INFO | train_inner | {"epoch": 12, "update": 11.708, "loss": "4.65", "nll_loss": "3.307", "ppl": "9.89", "wps": "63577.4", "ups": "17.8", "wpb": "3570.9", "bsz": "147", "num_updates": "12900", "lr": "0.00556846", "gnorm": "0.262", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "779"}
2023-12-13 23:44:02 | INFO | train_inner | {"epoch": 12, "update": 11.799, "loss": "4.64", "nll_loss": "3.295", "ppl": "9.82", "wps": "57915.8", "ups": "16.22", "wpb": "3571.2", "bsz": "141.2", "num_updates": "13000", "lr": "0.005547", "gnorm": "0.252", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "785"}
2023-12-13 23:44:09 | INFO | train_inner | {"epoch": 12, "update": 11.889, "loss": "4.636", "nll_loss": "3.291", "ppl": "9.78", "wps": "58543.1", "ups": "16.39", "wpb": "3571.4", "bsz": "144.5", "num_updates": "13100", "lr": "0.00552579", "gnorm": "0.257", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "791"}
2023-12-13 23:44:15 | INFO | train_inner | {"epoch": 12, "update": 11.98, "loss": "4.665", "nll_loss": "3.325", "ppl": "10.02", "wps": "56911.7", "ups": "16.12", "wpb": "3531.5", "bsz": "142.6", "num_updates": "13200", "lr": "0.00550482", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "797"}
2023-12-13 23:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:44:17 | INFO | valid | {"epoch": 12, "valid_loss": "4.513", "valid_nll_loss": "3.176", "valid_ppl": "9.04", "valid_wps": "127360", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "13222", "valid_best_loss": "4.513"}
2023-12-13 23:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 13222 updates
2023-12-13 23:44:17 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint12.pt
2023-12-13 23:44:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint12.pt
2023-12-13 23:44:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint12.pt (epoch 12 @ 13222 updates, score 4.513) (writing took 0.21744091995060444 seconds)
2023-12-13 23:44:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-12-13 23:44:17 | INFO | train | {"epoch": 12, "train_loss": "4.602", "train_nll_loss": "3.249", "train_ppl": "9.51", "train_wps": "58652.1", "train_ups": "16.37", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "13222", "train_lr": "0.00550024", "train_gnorm": "0.255", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "39", "train_wall": "800"}
2023-12-13 23:44:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:44:18 | INFO | fairseq.trainer | begin training epoch 13
2023-12-13 23:44:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:44:23 | INFO | train_inner | {"epoch": 13, "update": 12.071, "loss": "4.522", "nll_loss": "3.156", "ppl": "8.91", "wps": "45807", "ups": "12.78", "wpb": "3583.2", "bsz": "138.2", "num_updates": "13300", "lr": "0.00548408", "gnorm": "0.252", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "805"}
2023-12-13 23:44:28 | INFO | train_inner | {"epoch": 13, "update": 12.162, "loss": "4.499", "nll_loss": "3.13", "ppl": "8.76", "wps": "65831.2", "ups": "18.24", "wpb": "3609.9", "bsz": "151", "num_updates": "13400", "lr": "0.00546358", "gnorm": "0.246", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "811"}
2023-12-13 23:44:34 | INFO | train_inner | {"epoch": 13, "update": 12.252, "loss": "4.512", "nll_loss": "3.146", "ppl": "8.85", "wps": "64461.5", "ups": "17.8", "wpb": "3620.5", "bsz": "154.3", "num_updates": "13500", "lr": "0.00544331", "gnorm": "0.253", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "816"}
2023-12-13 23:44:39 | INFO | train_inner | {"epoch": 13, "update": 12.343, "loss": "4.504", "nll_loss": "3.137", "ppl": "8.8", "wps": "63875.2", "ups": "17.9", "wpb": "3569.4", "bsz": "151.8", "num_updates": "13600", "lr": "0.00542326", "gnorm": "0.245", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "822"}
2023-12-13 23:44:45 | INFO | train_inner | {"epoch": 13, "update": 12.434, "loss": "4.528", "nll_loss": "3.165", "ppl": "8.97", "wps": "65696.1", "ups": "17.89", "wpb": "3671.7", "bsz": "146.5", "num_updates": "13700", "lr": "0.00540343", "gnorm": "0.238", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "828"}
2023-12-13 23:44:50 | INFO | train_inner | {"epoch": 13, "update": 12.525, "loss": "4.561", "nll_loss": "3.205", "ppl": "9.22", "wps": "67490.6", "ups": "18.78", "wpb": "3593.1", "bsz": "149.1", "num_updates": "13800", "lr": "0.00538382", "gnorm": "0.259", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "833"}
2023-12-13 23:44:56 | INFO | train_inner | {"epoch": 13, "update": 12.615, "loss": "4.624", "nll_loss": "3.277", "ppl": "9.7", "wps": "60014.8", "ups": "16.77", "wpb": "3579.7", "bsz": "134.7", "num_updates": "13900", "lr": "0.00536442", "gnorm": "0.252", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "839"}
2023-12-13 23:45:02 | INFO | train_inner | {"epoch": 13, "update": 12.706, "loss": "4.555", "nll_loss": "3.196", "ppl": "9.17", "wps": "61702.4", "ups": "17.36", "wpb": "3553.4", "bsz": "149.6", "num_updates": "14000", "lr": "0.00534522", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "845"}
2023-12-13 23:45:08 | INFO | train_inner | {"epoch": 13, "update": 12.797, "loss": "4.593", "nll_loss": "3.243", "ppl": "9.47", "wps": "56847.3", "ups": "16.15", "wpb": "3519.1", "bsz": "144.6", "num_updates": "14100", "lr": "0.00532624", "gnorm": "0.251", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "851"}
2023-12-13 23:45:14 | INFO | train_inner | {"epoch": 13, "update": 12.887, "loss": "4.6", "nll_loss": "3.25", "ppl": "9.52", "wps": "58032.8", "ups": "16.08", "wpb": "3608.6", "bsz": "144.2", "num_updates": "14200", "lr": "0.00530745", "gnorm": "0.265", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "857"}
2023-12-13 23:45:21 | INFO | train_inner | {"epoch": 13, "update": 12.978, "loss": "4.684", "nll_loss": "3.348", "ppl": "10.18", "wps": "54205.2", "ups": "15.28", "wpb": "3548.4", "bsz": "129.8", "num_updates": "14300", "lr": "0.00528886", "gnorm": "0.263", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "864"}
2023-12-13 23:45:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:45:23 | INFO | valid | {"epoch": 13, "valid_loss": "4.527", "valid_nll_loss": "3.195", "valid_ppl": "9.16", "valid_wps": "128055", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "14324", "valid_best_loss": "4.513"}
2023-12-13 23:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 14324 updates
2023-12-13 23:45:23 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint13.pt
2023-12-13 23:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint13.pt
2023-12-13 23:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint13.pt (epoch 13 @ 14324 updates, score 4.527) (writing took 0.14797330368310213 seconds)
2023-12-13 23:45:24 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-12-13 23:45:24 | INFO | train | {"epoch": 13, "train_loss": "4.561", "train_nll_loss": "3.204", "train_ppl": "9.21", "train_wps": "59781", "train_ups": "16.68", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "14324", "train_lr": "0.00528443", "train_gnorm": "0.254", "train_loss_scale": "32", "train_train_wall": "63", "train_gb_free": "38.9", "train_wall": "866"}
2023-12-13 23:45:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:45:24 | INFO | fairseq.trainer | begin training epoch 14
2023-12-13 23:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:45:28 | INFO | train_inner | {"epoch": 14, "update": 13.069, "loss": "4.463", "nll_loss": "3.092", "ppl": "8.53", "wps": "48693.1", "ups": "13.92", "wpb": "3498.1", "bsz": "153.5", "num_updates": "14400", "lr": "0.00527046", "gnorm": "0.248", "loss_scale": "32", "train_wall": "5", "gb_free": "39", "wall": "871"}
2023-12-13 23:45:34 | INFO | train_inner | {"epoch": 14, "update": 13.16, "loss": "4.495", "nll_loss": "3.125", "ppl": "8.73", "wps": "56124.2", "ups": "15.86", "wpb": "3538.8", "bsz": "137.2", "num_updates": "14500", "lr": "0.00525226", "gnorm": "0.252", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "877"}
2023-12-13 23:45:40 | INFO | train_inner | {"epoch": 14, "update": 13.25, "loss": "4.511", "nll_loss": "3.146", "ppl": "8.85", "wps": "59217.5", "ups": "16.64", "wpb": "3557.8", "bsz": "148.9", "num_updates": "14600", "lr": "0.00523424", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "883"}
2023-12-13 23:45:46 | INFO | train_inner | {"epoch": 14, "update": 13.341, "loss": "4.489", "nll_loss": "3.12", "ppl": "8.7", "wps": "63531", "ups": "17.42", "wpb": "3646.9", "bsz": "136.3", "num_updates": "14700", "lr": "0.00521641", "gnorm": "0.244", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "889"}
2023-12-13 23:45:52 | INFO | train_inner | {"epoch": 14, "update": 13.432, "loss": "4.541", "nll_loss": "3.183", "ppl": "9.08", "wps": "56248.6", "ups": "15.9", "wpb": "3538.2", "bsz": "149.8", "num_updates": "14800", "lr": "0.00519875", "gnorm": "0.267", "loss_scale": "32", "train_wall": "6", "gb_free": "39", "wall": "895"}
2023-12-13 23:45:58 | INFO | train_inner | {"epoch": 14, "update": 13.523, "loss": "4.54", "nll_loss": "3.181", "ppl": "9.07", "wps": "61945.9", "ups": "17.55", "wpb": "3530.5", "bsz": "137.6", "num_updates": "14900", "lr": "0.00518128", "gnorm": "0.248", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "901"}
2023-12-13 23:46:04 | INFO | train_inner | {"epoch": 14, "update": 13.613, "loss": "4.518", "nll_loss": "3.155", "ppl": "8.91", "wps": "63763.3", "ups": "17.59", "wpb": "3624.2", "bsz": "151.3", "num_updates": "15000", "lr": "0.00516398", "gnorm": "0.253", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "906"}
2023-12-13 23:46:10 | INFO | train_inner | {"epoch": 14, "update": 13.704, "loss": "4.599", "nll_loss": "3.25", "ppl": "9.52", "wps": "60450.3", "ups": "16.74", "wpb": "3611.8", "bsz": "137.4", "num_updates": "15100", "lr": "0.00514685", "gnorm": "0.255", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "912"}
2023-12-13 23:46:15 | INFO | train_inner | {"epoch": 14, "update": 13.795, "loss": "4.492", "nll_loss": "3.129", "ppl": "8.75", "wps": "66452.8", "ups": "18.44", "wpb": "3603.4", "bsz": "158.1", "num_updates": "15200", "lr": "0.00512989", "gnorm": "0.242", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "918"}
2023-12-13 23:46:21 | INFO | train_inner | {"epoch": 14, "update": 13.886, "loss": "4.56", "nll_loss": "3.204", "ppl": "9.22", "wps": "58741.8", "ups": "16.31", "wpb": "3602.6", "bsz": "138.5", "num_updates": "15300", "lr": "0.0051131", "gnorm": "0.259", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "924"}
2023-12-13 23:46:27 | INFO | train_inner | {"epoch": 14, "update": 13.976, "loss": "4.545", "nll_loss": "3.19", "ppl": "9.12", "wps": "68714.3", "ups": "18.6", "wpb": "3694.8", "bsz": "156.7", "num_updates": "15400", "lr": "0.00509647", "gnorm": "0.253", "loss_scale": "32", "train_wall": "5", "gb_free": "38.9", "wall": "929"}
2023-12-13 23:46:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-12-13 23:46:30 | INFO | valid | {"epoch": 14, "valid_loss": "4.481", "valid_nll_loss": "3.15", "valid_ppl": "8.87", "valid_wps": "126744", "valid_wpb": "2835.3", "valid_bsz": "115.6", "valid_num_updates": "15426", "valid_best_loss": "4.481"}
2023-12-13 23:46:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 15426 updates
2023-12-13 23:46:30 | INFO | fairseq.trainer | Saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint14.pt
2023-12-13 23:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/jprado/experiments/nlp_experiments/checkpoint_d0_1e-0_99130/checkpoint14.pt
2023-12-13 23:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoint_d0_1e-0_99130/checkpoint14.pt (epoch 14 @ 15426 updates, score 4.481) (writing took 0.2161047412082553 seconds)
2023-12-13 23:46:30 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-12-13 23:46:30 | INFO | train | {"epoch": 14, "train_loss": "4.525", "train_nll_loss": "3.163", "train_ppl": "8.96", "train_wps": "59524.7", "train_ups": "16.61", "train_wpb": "3583.6", "train_bsz": "145.4", "train_num_updates": "15426", "train_lr": "0.00509218", "train_gnorm": "0.253", "train_loss_scale": "32", "train_train_wall": "64", "train_gb_free": "39", "train_wall": "933"}
2023-12-13 23:46:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1102
2023-12-13 23:46:30 | INFO | fairseq.trainer | begin training epoch 15
2023-12-13 23:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-12-13 23:46:34 | INFO | train_inner | {"epoch": 15, "update": 14.067, "loss": "4.432", "nll_loss": "3.056", "ppl": "8.32", "wps": "46543.8", "ups": "13.03", "wpb": "3572.5", "bsz": "156.7", "num_updates": "15500", "lr": "0.00508001", "gnorm": "0.253", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "937"}
2023-12-13 23:46:41 | INFO | train_inner | {"epoch": 15, "update": 14.158, "loss": "4.46", "nll_loss": "3.088", "ppl": "8.5", "wps": "53006.1", "ups": "15.3", "wpb": "3464.4", "bsz": "138.3", "num_updates": "15600", "lr": "0.0050637", "gnorm": "0.265", "loss_scale": "32", "train_wall": "6", "gb_free": "38.9", "wall": "944"}
